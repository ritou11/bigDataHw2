\documentclass[a4paper,12pt]{article}
\usepackage[noabs]{HaotianReport}
\usepackage[ruled]{algorithm2e}

\title{第二次作业：电影推荐}
\author{刘昊天}
\authorinfo{电博181班, 2018310648}
\runninghead{大数据分析(B)课程报告}
\studytime{2018年11月}

\graphicspath{{./}{./output/}}

\begin{document}
    \maketitle
    %\newpage
    \section{实验一:数据预处理}
    \paragraph{问题描述}
    将输入文件整理成唯独为用户*电影的矩阵$X$，其中$X(i,j)$为用户$i$对电影$j$的打分。输出两个矩阵：$X_{train}$和$X_{test}$，分别对应训练集和测试集。

    定义集合$U$为用户集合，共$N_u$个用户；集合$M$为电影集合，共$N_m$个电影。$X_{ij}$为用户$i\in U$对电影$j\in M$的评分。在本题中，$N_u=10000$，$N_m=10000$。

    在本题中，共提供了两组数据，其中训练集共6897746条记录，则$X_{train}$至多有$6.90\%$的非零元素；测试集共1719466条记录，$X_{test}$至多有$1.72\%$的非零元素。可见，$X_{train}$和$X_{test}$是十分稀疏的，因此需要用稀疏技术进行处理。

    在Python的scipy包中，有sparse模块包含了不同的稀疏矩阵类。其中csr\_matrix (Compressed Sparse Row marix) 为按行压缩的稀疏矩阵格式，另外还有csc(Compressed Sparse Column)、coo(COOrdinate)、bsr(Block Sparse Row)、dia(DIAgonal)、dok(Dictionary of Keys)、lil(List of Lists)等储存格式。不同的稀疏矩阵格式适用于不同的使用场景，各有优势劣势。csr\_matrix的优点在于高效的算术运算(CSR + CSR，CSR * CSR等)、行切片以及矩阵矢量积，而劣势是列切片及稀疏结构的变化。在本题中，数据矩阵不发生结构变化，且需参与大量运算，因此csr\_matrix是合适的。

    考查数据文件的格式发现，训练集、测试集的数据文件中用户uid是一个大整数，而我们需要将其映射到一个[0,9999]的区间中。这个映射是通过用户数据文件建立的，我们可以将用户出现在该文件中的行号减一作为用户在矩阵中的新id，记为$i$。由于数据文件行数很多，构建矩阵过程中需要大量查询用户的$i$值，因此映射$uid\rightarrow i$需要是高效的。从数据结构出发可以考虑使用HashMap，而在Python中只需要使用dict即可。建立该映射的代码如\cref{lst:readUserid}所示。电影mid在矩阵中的新id即为$j$，$j=mid-1$即可。
    \begin{lstlisting}[language=python,caption={readUserid},label=lst:readUserid]
def __init__(self, filename = ''):
    self.user_id = dict()
    if filename:
        self.readUserid(filename)
def readUserid(self, filename):
    with open(filename, 'r') as f:
        for i, l in enumerate(f):
            self.user_id[int(l)] = i
    \end{lstlisting}

    考虑到csr格式的劣势，我们不能采用逐个增加的方法构建稀疏矩阵，而应采用一次构建的方式。最终使用的生成代码如\cref{lst:getMatrixFromTxt}所示。
    \begin{lstlisting}[language=python,caption={getMatrixFromTxt},label=lst:getMatrixFromTxt]
# user index = row_number of user_id (from 0)
# movie index = movie_id - 1 (from 0)
def getMatrixFromTxt(self, filename, level=0):
    uid = list()
    mid = list()
    sc = list()
    with open(filename, 'r') as f:
        for i, l in enumerate(f):
            dt = l.split()
            score = int(dt[2]) - level
            if score != 0:
                uid.append(self.user_id[int(dt[0])])
                mid.append(int(dt[1]) - 1)
                sc.append(score)
    return csr_matrix((sc, (uid, mid)), shape=(hg.N, hg.N))
    \end{lstlisting}

    程序中，level参数表示构建矩阵时考虑的水平参数，读取的每个评分都将被减去level。从意义上将，在level代表了用户对没有打分的电影的评价，也即一个整体评价水平。这个参数是本报告引入的\textbf{关键参数}，将极大影响最终结果。

    在MacBook Pro(3.1 GHz Intel Core i5, 16GB 2133 MHz LPDDR3, macOS Mojave 10.14.1)平台上进行测试，Python版本为3.6.2。选择全量数据，生成矩阵耗时如\cref{tbl:exp1}所示，其中储存矩阵采用pickle包的dump函数进行储存。
    \begin{table}
      \centering
      \caption{生成数据矩阵耗时}
      \label{tbl:exp1}
      \begin{tabular}{ll}
        \toprule
        操作&用时\\
        \midrule
        构建测试矩阵&3514.88ms\\
        构建训练矩阵&13491.16ms\\
        储存矩阵&263.16ms\\
        \bottomrule
      \end{tabular}
    \end{table}
    \section{实验二:协同过滤}
    \paragraph{问题描述}
    实现基于用户的协同过滤算法：猜测用户$i$是否喜欢电影$j$，只要看与$i$相似的用户是否喜欢$j$。与$i$越相似的用户，其对j的评分越有参考价值。
    \subsection{原理推导}
    根据原理写出
    \begin{equation}
      \bar S_{ij} = \frac{\sum\limits_{k\in U} Q_{ik}S_{kj}}{\sum\limits_{k\in U} |Q_{ik}|}
    \end{equation}
    其中，$Q_{N_u\times N_u}$为相似度矩阵，$Q=Q^T$，$S_{N_u\times N_m}$为已知用户电影评分矩阵，$\bar S_{N_u\times N_m}$为估计用户电影评分矩阵。在本题中，$S=X_{train}, T=X_{test}$。

    定义信息矩阵$A_{N_u\times N_m}$
    \begin{equation}
      A_{ij}=\begin{cases}
      1, & \text{if } S_{ij} \text{ is known}\\
      0, & \text{otherwise}
    \end{cases}
    \end{equation}
    根据以上表达，可以写出矩阵形式
    \begin{equation}
      \bar S = QS\circ [|Q|A]^{-1}
    \end{equation}
    其中$<\circ>$运算符代表矩阵逐元素乘法，$[\cdot]^{-1}$代表矩阵逐元素取倒数。

    相似度矩阵采用余弦相似度的计算方式，即
    \begin{equation}
      \begin{aligned}
        Q_{ij} =
        \begin{cases}
          \frac{S_i\cdot S_j}{||S_i||_2 ||S_j||_2} = \frac{S_i S_j^T}{\sqrt{S_i S_i^T} \sqrt{S_j S_j^T}},& \text{if } i\neq j, S_i\neq 0, S_j\neq 0\\
          0, & \text{if } i\neq j, S_i=0 \text{ or } S_j=0\\
          1, & \text{if } i= j
        \end{cases}
      \end{aligned}
    \end{equation}
    其中$S_i$表示$S$阵的第$i$行。

    根据该表达，可以写出矩阵形式的相似度计算
    \begin{equation}
      \begin{aligned}
        Q' &= SS^T\\
        Q'_{ii} &= 1\\
        Q &= diag(Q')^{-1}Q'diag^{-1}(Q')^{-1}
      \end{aligned}
    \end{equation}
    其中$diag()^{-1}$代表矩阵的对角元素的倒数组成的对角矩阵。

    计算得到预测值后，使用RMSE指标评估算法的准确性。具体做法是，对于测试集的每个元素，求取预测值和实际值的偏差，并将其方均根作为误差值。定义信息矩阵$B_{N_u\times N_m}$，
    \begin{equation}
      B_{ij}=\begin{cases}
      1, & \text{if } T_{ij} \text{ is known}\\
      0, & \text{otherwise}
    \end{cases}
    \end{equation}
    则RMSE的表达为
    \begin{equation}
      \text{RMSE}=\sqrt{\frac{\sum\limits_{<i,j>,B_{ij}=1} (T_{ij} - \bar S_{ij})}{n}}
    \end{equation}
    写成矩阵形式
    \begin{equation}
      \text{RMSE}=||T - \bar S\circ B||_2/\sqrt{n}
    \end{equation}
    \subsection{算法实现}
    在本任务中，首先考虑最简单的$level=0$情况，此时相当于认为用户对未评分的电影评分为0。基本代码见附件exp2\_org.py及exp2.py，思路按照上文所述。其中涉及两个技巧，其一是矩阵信息数据，采用pickle读取实验一中生成的文件，
    其二是信息矩阵的生成，可以采用
    \begin{lstlisting}[language=python]
mask = (trainMatrix != 0).astype(int)
testMask = (testMatrix != 0).astype(int)
    \end{lstlisting}

    特别地，文件exp2\_org.py中给出了一种利用测试阵稀疏性的求解方式。这种方式首先同样求解相似度矩阵，是一个$N_u\times N_u$的满阵，再对于测试集的每个元素分别求预测值，而不使用矩阵乘法一次将全部$N_u\times N_m$个预测值全部求出。由于测试集的稀疏度很高(1.72\%非零元素)，理论上讲这种方法会节省大量计算时间，且节省内存空间。但在实际操作中，由于这种方法必须采用循环方式求解，无法利用Python矩阵运算的底层代码，因此时间效率会受到影响。究竟是Python矩阵运算底层优化带来的效率提升有效，还是1.72\%的稀疏性带来的运算减少有效，需要通过实际验证。

    进一步考虑，认为用户对未评分的电影评分为0显然是不合理的，且目前求出的相似度矩阵均为正数，无法体现对不相似用户的评分的惩罚。由于用户的评分是[1,5]，一个初等的想法是将所有未知评分都置为3，但这样会使得训练矩阵丧失稀疏性，大大增加计算量，且所有相似度依然为正数。因此，可以将评分的指标中心置为3，在所有已知评分的基础上减3，以新的评分矩阵计算相似度。这就是所谓的$level=3$即"L3方法"。此时矩阵会由于$level=3$而出现新的0元素，因此稀疏性将会增强，计算效率提高。

    L3方法的代码见附件exp2\_L3.py。其中关键步骤为
    \begin{lstlisting}[language=python]
upper = simiMat * trainMatrixL3
mask = (trainMatrix != 0).astype(int)
lower = np.abs(simiMat) * mask
...
maskPredMat = predMat.multiply(testMask) + testMask.multiply(3)
    \end{lstlisting}
    即应注意在给出预测分数时要将$level$补回。其中trainMatrixL3的生成只需将生成矩阵函数的$level$参数设为3。

    更进一步考虑，采用3作为对未评分的的电影评分也是不够准确的。对不同用户来说，打分的标准不尽相同。有些人会倾向于给高分，而有些人则倾向于给低分，那么对不同人的未评分电影评分也应该是有差异的。因此，可以考虑采用每个用户的平均评分，来作为对未评分项的估计。为保持稀疏度，同样采用类似L3方法的技巧，将用户的已有评分减去该用户的评分平均值，相当于做了一个标准化的操作。这种方法为"LM方法"，即$level_i=Mean_i$。此时稀疏性不会得到增强。

    LM方法的代码见附件exp2\_LM.py。其中关键步骤为
    \begin{lstlisting}[language=python]
mask = (trainMatrix != 0).astype(int)
trainRowMeans = list()
for i in range(hg.N):
    trainRowMeans.append(trainMatrix.getrow(i).sum() / trainMatrix.getrow(i).nnz)
trainMean = diags(trainRowMeans, shape=(hg.N, hg.N))
trainMatrixLM = trainMatrix - trainMean * mask
simiMat = csr_cosine_similarity(trainMatrixLM)
...
upper = simiMat * trainMatrixLM
lower = np.abs(simiMat) * mask
...
maskPredMat = predMat.multiply(testMask) + trainMean * testMask
    \end{lstlisting}
    即应注意在给出预测分数时要将$level_i$补回。

    L3方法和LM方法中，特别需要注意的是，信息矩阵应使用原始矩阵进行导出，这是信息矩阵定义的一个显然推论。
    \subsection{结果分析}
    计算结果如\cref{tbl:exp2}所示。
    \begin{table}
      \centering
      \caption{协同过滤算法结果对比}
      \label{tbl:exp2}
      \begin{tabular}{lrrrrr}
        \toprule
        算法&分子计算时间&分母计算时间&预测时间&总时间&RMSE\\
        \midrule
        Original & - & - & - & - & - \\
        Standard & - & - & - & - & - \\
        L3 & - & - & - & - & - \\
        LM & - & - & - & - & - \\
        \bottomrule
      \end{tabular}
    \end{table}
    \begin{enumerate}
      \item Python矩阵运算底层优化带来的效率提升要比1.72\%的稀疏性带来的运算减少有效得多，效率大致提升TODO倍。可见，Python矩阵运算库经过了十分精细、强大的优化，更适合用在这里。
      \item L3方法与LM方法带来的效果改进是显著的。
      \item L3方法由于提升了训练集的稀疏度，因此分子的计算效率更高；由于其使用的信息矩阵是通过原矩阵求出的，因此分母计算时间几乎不变。
    \end{enumerate}
    \section{实验三:矩阵分解}
    \paragraph{问题描述}
    实现基于梯度下降的矩阵分解算法：将行为矩阵$X$分解为$U$和$V$两个矩阵的乘积，使$UV^T$在已知值部分逼近$X$。隐空间维度$k$是算法的参数，$U$和$V$可以认为是用户和电影在隐空间的特征表达，其乘积矩阵可预测$X$的未知部分。
    $$
      X_{N_u\times N_m} = U_{N_u\times k}V_{N_m\times k}^T
    $$
    \subsection{原理推导}
    根据题目提供的信息，目标函数如\cref{eq:exp3J}所示。本算法的核心就是通过迭代的方式，使得$J$最小，此时则认为$UV^T$对原矩阵$X$的拟合最佳。
    \begin{equation}
      \label{eq:exp3J}
      J =\frac{1}{2} ||A\circ (X-UV^T)||_F^2 + \lambda ||U||_F^2 + \lambda ||V||_F^2
    \end{equation}

    对目标函数求偏导，得到迭代求解需要的表达式
    \begin{equation}
      \begin{aligned}
        \partial J/\partial U &= (A\circ (UV^T-X))V + 2\lambda U\\
        \partial J/\partial V &= (A\circ (UV^T-X))^TU + 2\lambda V\\
      \end{aligned}
    \end{equation}

    写出基于Jacobi迭代的UV分解算法，如\cref{alg:jacobi}所示。
    \begin{algorithm}
        \caption{矩阵的UV分解算法(Jacobi)}
        \label{alg:jacobi}
        \KwIn{学习率$\alpha$，隐空间维度$k$，正则项系数$\lambda$，原矩阵$X$，信息矩阵$A$，UV初始值幅度$e$，收敛条件$\epsilon$}
        \KwOut{矩阵的UV分解$X\approx UV^T$}

        initialize $U_0=Rand_{N_u,k}e, V_0=Rand_{N_m,k}e, J_0=0, dJ = \epsilon + 1, i=0$\;
        \While{$dJ > \epsilon$}{
            $U_{i+1}=U_i-\alpha ((AU_iV_i^T-X)V_i+2\lambda U_i)$\;
            $V_{i+1}=V_i-\alpha ((AU_iV_i^T-X)^TU_i+2\lambda V_i)$\;
            $J_{i+1} =||AU_{i+1}V_{i+1}^T-X||_2^2/2 + \lambda ||U_{i+1}||_2^2 + \lambda ||V_{i+1}||_F^2$\;
            $dJ = J_{i} - J_{i+1}$\;
            $i = i + 1$\;
        }
    \end{algorithm}

    写出基于Gauss-Seidel迭代的UV分解算法，如\cref{alg:gauss}所示。
    \begin{algorithm}
        \caption{矩阵的UV分解算法(Gauss-Seidel)}
        \label{alg:gauss}
        \KwIn{学习率$\alpha$，隐空间维度$k$，正则项系数$\lambda$，原矩阵$X$，信息矩阵$A$，UV初始值幅度$e$，收敛条件$\epsilon$}
        \KwOut{矩阵的UV分解$X\approx UV^T$}

        initialize $U_0=Rand_{N_u,k}e, V_0=Rand_{N_m,k}e, J_0=0, dJ = \epsilon + 1, i=0$\;
        \While{$dJ > \epsilon$}{
            $U_{i+1}=U_i-\alpha ((AU_iV_i^T-X)V_i+2\lambda U_i)$\;
            $V_{i+1}=V_i-\alpha ((AU_{i+1}V_i^T-X)U_{i+1}+2\lambda V_i)$\;
            $J_{i+1} =||AU_{i+1}V_{i+1}^T-X||_2^2/2 + \lambda ||U_{i+1}||_2^2 + \lambda ||V_{i+1}||_F^2$\;
            $dJ = J_{i} - J_{i+1}$\;
            $i = i + 1$\;
        }
    \end{algorithm}

    Gauss-Seidel算法与Jacobi算法的区别在于，前者在每步中，每次更新各个变量时，总是使用最新的结果。一般来讲，Gauss-Seidel迭代的收敛速度要比Jacobi更好，但具体到本题需要通过实验验证效率情况。
    \subsection{算法实现}

    \subsection{结果分析}

    \label{applastpage}
    \newpage
    \bibliographystyle{ieeetr}
    \bibliography{report}
\iffalse
\begin{itemize}[noitemsep,topsep=0pt]
%no white space
\end{itemize}
\begin{enumerate}[label=\Roman{*}.,noitemsep,topsep=0pt]
%use upper case roman
\end{enumerate}
\begin{multicols}{2}
%two columns
\end{multicols}
\fi
\end{document}

\documentclass[a4paper,12pt]{article}
\usepackage[noabs]{HaotianReport}
\usepackage[ruled]{algorithm2e}

\title{第二次作业：电影推荐}
\author{刘昊天}
\authorinfo{电博181班, 2018310648}
\runninghead{大数据分析(B)课程报告}
\studytime{2018年11月}

\graphicspath{{./}{../output/}}

\begin{document}
    \maketitle
    %\newpage
    \section{实验一:数据预处理}
    \paragraph{问题描述}
    将输入文件整理成唯独为用户*电影的矩阵$X$，其中$X(i,j)$为用户$i$对电影$j$的打分。输出两个矩阵：$X_{train}$和$X_{test}$，分别对应训练集和测试集。

    定义集合$U$为用户集合，共$N_u$个用户；集合$M$为电影集合，共$N_m$个电影。$X_{ij}$为用户$i\in U$对电影$j\in M$的评分。在本题中，$N_u=10000$，$N_m=10000$。

    在本题中，共提供了两组数据，其中训练集共6897746条记录，则$X_{train}$至多有$6.90\%$的非零元素；测试集共1719466条记录，$X_{test}$至多有$1.72\%$的非零元素。可见，$X_{train}$和$X_{test}$是十分稀疏的，因此需要用稀疏技术进行处理。

    在Python的scipy包中，有sparse模块包含了不同的稀疏矩阵类。其中csr\_matrix (Compressed Sparse Row marix) 为按行压缩的稀疏矩阵格式，另外还有csc(Compressed Sparse Column)、coo(COOrdinate)、bsr(Block Sparse Row)、dia(DIAgonal)、dok(Dictionary of Keys)、lil(List of Lists)等储存格式。不同的稀疏矩阵格式适用于不同的使用场景，各有优势劣势。csr\_matrix的优点在于高效的算术运算(CSR + CSR，CSR * CSR等)、行切片以及矩阵矢量积，而劣势是列切片及稀疏结构的变化。在本题中，数据矩阵不发生结构变化，且需参与大量运算，因此csr\_matrix是合适的。

    考查数据文件的格式发现，训练集、测试集的数据文件中用户uid是一个大整数，而我们需要将其映射到一个[0,9999]的区间中。这个映射是通过用户数据文件建立的，我们可以将用户出现在该文件中的行号减一作为用户在矩阵中的新id，记为$i$。由于数据文件行数很多，构建矩阵过程中需要大量查询用户的$i$值，因此映射$uid\rightarrow i$需要是高效的。从数据结构出发可以考虑使用HashMap，而在Python中只需要使用dict即可。建立该映射的代码如\cref{lst:readUserid}所示。电影mid在矩阵中的新id即为$j$，$j=mid-1$即可。
    \begin{lstlisting}[language=python,caption={readUserid},label=lst:readUserid]
def __init__(self, filename = ''):
    self.user_id = dict()
    if filename:
        self.readUserid(filename)
def readUserid(self, filename):
    with open(filename, 'r') as f:
        for i, l in enumerate(f):
            self.user_id[int(l)] = i
    \end{lstlisting}

    考虑到csr格式的劣势，我们不能采用逐个增加的方法构建稀疏矩阵，而应采用一次构建的方式。最终使用的生成代码如\cref{lst:getMatrixFromTxt}所示。
    \begin{lstlisting}[language=python,caption={getMatrixFromTxt},label=lst:getMatrixFromTxt]
# user index = row_number of user_id (from 0)
# movie index = movie_id - 1 (from 0)
def getMatrixFromTxt(self, filename, level=0):
    uid = list()
    mid = list()
    sc = list()
    with open(filename, 'r') as f:
        for i, l in enumerate(f):
            dt = l.split()
            score = int(dt[2]) - level
            if score != 0:
                uid.append(self.user_id[int(dt[0])])
                mid.append(int(dt[1]) - 1)
                sc.append(score)
    return csr_matrix((sc, (uid, mid)), shape=(hg.N, hg.N))
    \end{lstlisting}

    程序中，level参数表示构建矩阵时考虑的水平参数，读取的每个评分都将被减去level。从意义上将，在level代表了用户对没有打分的电影的评价，也即一个整体评价水平。这个参数是本报告引入的\textbf{关键参数}，将极大影响最终结果。

    在MacBook Pro(3.1 GHz Intel Core i5, 16GB 2133 MHz LPDDR3, macOS Mojave 10.14.1)平台上进行测试，Python版本为3.6.2。选择全量数据，生成矩阵耗时如\cref{tbl:exp1}所示，其中储存矩阵采用pickle包的dump函数进行储存。
    \begin{table}
      \centering
      \caption{生成数据矩阵耗时}
      \label{tbl:exp1}
      \begin{tabular}{ll}
        \toprule
        操作&用时\\
        \midrule
        构建测试矩阵&3514.88ms\\
        构建训练矩阵&13491.16ms\\
        储存矩阵&263.16ms\\
        \bottomrule
      \end{tabular}
    \end{table}
    \section{实验二:协同过滤}
    \paragraph{问题描述}
    实现基于用户的协同过滤算法：猜测用户$i$是否喜欢电影$j$，只要看与$i$相似的用户是否喜欢$j$。与$i$越相似的用户，其对j的评分越有参考价值。
    \subsection{原理推导}
    \label{exp2eq}
    根据原理写出
    \begin{equation}
      \bar S_{ij} = \frac{\sum\limits_{k\in U} Q_{ik}S_{kj}}{\sum\limits_{k\in U} |Q_{ik}|}
    \end{equation}
    其中，$Q_{N_u\times N_u}$为相似度矩阵，$Q=Q^T$，$S_{N_u\times N_m}$为已知用户电影评分矩阵，$\bar S_{N_u\times N_m}$为估计用户电影评分矩阵。在本题中，$S=X_{train}, T=X_{test}$。

    定义信息矩阵$A_{N_u\times N_m}$
    \begin{equation}
      A_{ij}=\begin{cases}
      1, & \text{if } S_{ij} \text{ is known}\\
      0, & \text{otherwise}
    \end{cases}
    \end{equation}
    根据以上表达，可以写出矩阵形式
    \begin{equation}
      \bar S = QS\circ [|Q|A]^{-1}
    \end{equation}
    其中$<\circ>$运算符代表矩阵逐元素乘法，$[\cdot]^{-1}$代表矩阵逐元素取倒数。

    相似度矩阵采用余弦相似度的计算方式，即
    \begin{equation}
      \begin{aligned}
        Q_{ij} =
        \begin{cases}
          \frac{S_i\cdot S_j}{||S_i||_2 ||S_j||_2} = \frac{S_i S_j^T}{\sqrt{S_i S_i^T} \sqrt{S_j S_j^T}},& \text{if } i\neq j, S_i\neq 0, S_j\neq 0\\
          0, & \text{if } i\neq j, S_i=0 \text{ or } S_j=0\\
          1, & \text{if } i= j
        \end{cases}
      \end{aligned}
    \end{equation}
    其中$S_i$表示$S$阵的第$i$行。

    根据该表达，可以写出矩阵形式的相似度计算
    \begin{equation}
      \begin{aligned}
        Q' &= SS^T\\
        Q'_{ii} &= 1\\
        Q &= diag(Q')^{-1}Q'diag^{-1}(Q')^{-1}
      \end{aligned}
    \end{equation}
    其中$diag()^{-1}$代表矩阵的对角元素的倒数组成的对角矩阵。

    计算得到预测值后，使用RMSE指标评估算法的准确性。具体做法是，对于测试集的每个元素，求取预测值和实际值的偏差，并将其方均根作为误差值。定义信息矩阵$B_{N_u\times N_m}$，
    \begin{equation}
      B_{ij}=\begin{cases}
      1, & \text{if } T_{ij} \text{ is known}\\
      0, & \text{otherwise}
    \end{cases}
    \end{equation}
    则RMSE的表达为
    \begin{equation}
      \text{RMSE}=\sqrt{\frac{\sum\limits_{<i,j>,B_{ij}=1} (T_{ij} - \bar S_{ij})}{n}}
    \end{equation}
    写成矩阵形式
    \begin{equation}
      \text{RMSE}=||T - \bar S\circ B||_2/\sqrt{n}
    \end{equation}
    \subsection{算法实现}
    \label{exp2alg}
    在本任务中，首先考虑最简单的$level=0$情况，此时相当于认为用户对未评分的电影评分为0。基本代码见附件exp2\_org.py及exp2.py，思路按照上文所述。其中涉及两个技巧，其一是矩阵信息数据，采用pickle读取实验一中生成的文件，
    其二是信息矩阵的生成，可以采用
    \begin{lstlisting}[language=python]
mask = (trainMatrix != 0).astype(int)
testMask = (testMatrix != 0).astype(int)
    \end{lstlisting}

    特别地，文件exp2\_org.py中给出了一种利用测试阵稀疏性的求解方式。这种方式首先同样求解相似度矩阵，是一个$N_u\times N_u$的满阵，再对于测试集的每个元素分别求预测值，而不使用矩阵乘法一次将全部$N_u\times N_m$个预测值全部求出。由于测试集的稀疏度很高(1.72\%非零元素)，理论上讲这种方法会节省大量计算时间，且节省内存空间。但在实际操作中，由于这种方法必须采用循环方式求解，无法利用Python矩阵运算的底层代码，因此时间效率会受到影响。究竟是Python矩阵运算底层优化带来的效率提升有效，还是1.72\%的稀疏性带来的运算减少有效，需要通过实际验证。

    进一步考虑，认为用户对未评分的电影评分为0显然是不合理的，且目前求出的相似度矩阵均为正数，无法体现对不相似用户的评分的惩罚。由于用户的评分是[1,5]，一个初等的想法是将所有未知评分都置为3，但这样会使得训练矩阵丧失稀疏性，大大增加计算量，且所有相似度依然为正数。因此，可以将评分的指标中心置为3，在所有已知评分的基础上减3，以新的评分矩阵计算相似度。这就是所谓的$level=3$即"L3方法"。此时矩阵会由于$level=3$而出现新的0元素，因此稀疏性将会增强，计算效率提高。

    L3方法的代码见附件exp2\_L3.py。其中关键步骤为
    \begin{lstlisting}[language=python]
upper = simiMat * trainMatrixL3
mask = (trainMatrix != 0).astype(int)
lower = np.abs(simiMat) * mask
...
maskPredMat = predMat.multiply(testMask) + testMask.multiply(3)
    \end{lstlisting}
    即应注意在给出预测分数时要将$level$补回。其中trainMatrixL3的生成只需将生成矩阵函数的$level$参数设为3。

    更进一步考虑，采用3作为对未评分的的电影评分也是不够准确的。对不同用户来说，打分的标准不尽相同。有些人会倾向于给高分，而有些人则倾向于给低分，那么对不同人的未评分电影评分也应该是有差异的。因此，可以考虑采用每个用户的平均评分，来作为对未评分项的估计。为保持稀疏度，同样采用类似L3方法的技巧，将用户的已有评分减去该用户的评分平均值，相当于做了一个标准化的操作。这种方法为"LM方法"，即$level_i=Mean_i$。此时稀疏性不会得到增强。

    LM方法的代码见附件exp2\_LM.py。其中关键步骤为
    \begin{lstlisting}[language=python]
mask = (trainMatrix != 0).astype(int)
trainRowMeans = list()
for i in range(hg.N):
    trainRowMeans.append(trainMatrix.getrow(i).sum() / trainMatrix.getrow(i).nnz)
trainMean = diags(trainRowMeans, shape=(hg.N, hg.N))
trainMatrixLM = trainMatrix - trainMean * mask
simiMat = csr_cosine_similarity(trainMatrixLM)
...
upper = simiMat * trainMatrixLM
lower = np.abs(simiMat) * mask
...
maskPredMat = predMat.multiply(testMask) + trainMean * testMask
    \end{lstlisting}
    即应注意在给出预测分数时要将$level_i$补回。

    L3方法和LM方法中，特别需要注意的是，信息矩阵应使用原始矩阵进行导出，这是信息矩阵定义的一个显然推论。
    \subsection{结果分析}
    计算结果如\cref{tbl:exp2}所示。
    \begin{table}
      \centering
      \caption{协同过滤算法结果对比}
      \label{tbl:exp2}
      \begin{tabular}{lrrrrr}
        \toprule
        算法&分子计算时间&分母计算时间&预测时间&总时间&RMSE\\
        \midrule
        Original & - & - & - & 2111999.77ms & 1.02 \\
        Standard & 139579.92ms & 143101.54ms & 7331.79ms & 290013.25ms & 1.02 \\
        L3 & 91306.96ms & 139133.05ms & 7889.58ms & 238329.59ms & 0.93 \\
        LM & 133149.89ms & 137770.19ms & 7711.46ms & 278631.54ms & 0.87 \\
        \bottomrule
      \end{tabular}
    \end{table}
    \begin{enumerate}
      \item Python矩阵运算底层优化带来的效率提升要比1.72\%的稀疏性带来的运算减少有效得多，效率大致提升TODO倍。可见，Python矩阵运算库经过了十分精细、强大的优化，更适合用在这里。
      \item L3方法与LM方法带来的效果改进是显著的。
      \item L3方法由于提升了训练集的稀疏度，因此分子的计算效率更高；由于其使用的信息矩阵是通过原矩阵求出的，因此分母计算时间几乎不变。
    \end{enumerate}
    \section{实验三:矩阵分解}
    \paragraph{问题描述}
    实现基于梯度下降的矩阵分解算法：将行为矩阵$X$分解为$U$和$V$两个矩阵的乘积，使$UV^T$在已知值部分逼近$X$。隐空间维度$k$是算法的参数，$U$和$V$可以认为是用户和电影在隐空间的特征表达，其乘积矩阵可预测$X$的未知部分。
    $$
      X_{N_u\times N_m} = U_{N_u\times k}V_{N_m\times k}^T
    $$
    \subsection{原理推导}
    根据题目提供的信息，目标函数如\cref{eq:exp3J}所示。本算法的核心就是通过迭代的方式，使得$J$最小，此时则认为$UV^T$对原矩阵$X$的拟合最佳。
    \begin{equation}
      \label{eq:exp3J}
      J =\frac{1}{2} ||A\circ (X-UV^T)||_F^2 + \lambda ||U||_F^2 + \lambda ||V||_F^2
    \end{equation}

    对目标函数求偏导，得到迭代求解需要的表达式
    \begin{equation}
      \begin{aligned}
        \partial J/\partial U &= (A\circ (UV^T-X))V + 2\lambda U\\
        \partial J/\partial V &= (A\circ (UV^T-X))^TU + 2\lambda V\\
      \end{aligned}
    \end{equation}

    写出基于Jacobi迭代的UV分解算法，如\cref{alg:jacobi}所示。
    \begin{algorithm}
        \caption{矩阵的UV分解算法(Jacobi)}
        \label{alg:jacobi}
        \KwIn{学习率$\alpha$，隐空间维度$k$，正则项系数$\lambda$，原矩阵$X$，信息矩阵$A$，UV初始值幅度$e$，收敛条件$\epsilon$}
        \KwOut{矩阵的UV分解$X\approx UV^T$}

        initialize $U_0=Rand_{N_u,k}e, V_0=Rand_{N_m,k}e, J_0=0, dJ = \epsilon + 1, i=0$\;
        \While{$dJ > \epsilon$}{
            $U_{i+1}=U_i-\alpha ((AU_iV_i^T-X)V_i+2\lambda U_i)$\;
            $V_{i+1}=V_i-\alpha ((AU_iV_i^T-X)^TU_i+2\lambda V_i)$\;
            $J_{i+1} =||AU_{i+1}V_{i+1}^T-X||_2^2/2 + \lambda ||U_{i+1}||_2^2 + \lambda ||V_{i+1}||_F^2$\;
            $dJ = J_{i} - J_{i+1}$\;
            $i = i + 1$\;
        }
    \end{algorithm}

    写出基于Gauss-Seidel迭代的UV分解算法，如\cref{alg:gauss}所示。
    \begin{algorithm}
        \caption{矩阵的UV分解算法(Gauss-Seidel)}
        \label{alg:gauss}
        \KwIn{学习率$\alpha$，隐空间维度$k$，正则项系数$\lambda$，原矩阵$X$，信息矩阵$A$，UV初始值幅度$e$，收敛条件$\epsilon$}
        \KwOut{矩阵的UV分解$X\approx UV^T$}

        initialize $U_0=Rand_{N_u,k}e, V_0=Rand_{N_m,k}e, J_0=0, dJ = \epsilon + 1, i=0$\;
        \While{$dJ > \epsilon$}{
            $U_{i+1}=U_i-\alpha ((AU_iV_i^T-X)V_i+2\lambda U_i)$\;
            $V_{i+1}=V_i-\alpha ((AU_{i+1}V_i^T-X)U_{i+1}+2\lambda V_i)$\;
            $J_{i+1} =||AU_{i+1}V_{i+1}^T-X||_2^2/2 + \lambda ||U_{i+1}||_2^2 + \lambda ||V_{i+1}||_F^2$\;
            $dJ = J_{i} - J_{i+1}$\;
            $i = i + 1$\;
        }
    \end{algorithm}

    Gauss-Seidel算法与Jacobi算法的区别在于，前者在每步中，每次更新各个变量时，总是使用最新的结果。一般来讲，Gauss-Seidel迭代的收敛速度要比Jacobi更好，但具体到本题需要通过实验验证效率情况。

    根据该原理，令待分解矩阵$X=S$，则预测矩阵为$\bar S=UV^T$。利用与\cref{exp2eq}相同的计算方式，可以得到本方法的RMSE。

    同时，\cref{exp2alg}中提出的L3方法及LM方法，由于改善的是输入矩阵$S$的合理性，因此在此处同样适用。
    \subsection{算法实现}
    根据\cref{alg:gauss,alg:jacobi}写出相应Python代码即可。为了提高计算效率，需要使用一些技巧。
    \begin{enumerate}
      \item 缓存，使用内存空间换取重复计算的时间。由于大型矩阵运算计算量很大，因此应该将一些中间变量储存下来，避免每一步重复计算。如此处则缓存了AUVTX代表$A\circ (X-UV^T)$。
    \begin{lstlisting}[language=python,breaklines=true]
for i in range(MAX_ITER):
  pJU = AUVTX * V + 2 * lbd * U
  pJV = AUVTX.T * U + 2 * lbd * V
  U = U - alpha * pJU
  V = V - alpha * pJV
  UVT = U * V.T
  AUVT = A.multiply(UVT)
  AUVTX = AUVT - trainMatrix
  rmse = norm(mask.multiply(UVT) - testMatrix) / np.sqrt(n)
  JL = J
  J = 0.5 * norm(AUVTX)**2 + lbd * np.linalg.norm(U)**2 + lbd * np.linalg.norm(V)**2
  if JL - J < eps:
      break
    \end{lstlisting}
      \item 在迭代步骤中，我们计算RMSE作为作业参考，但算法本身不应使用RMSE作为收敛判据的一部分，即在训练过程中不能利用任何与测试集相关的信息。
      \item 设置最大迭代次数防止迭代程序跑飞。
      \item 由于本实验中对比Gauss-Seidel、Jacobi迭代方法，以及L3、LM等改进方法，因此一些常用的输入矩阵应进行缓存，使用硬盘空间换取每次的计算时间。
    \end{enumerate}

    \subsection{结果分析}
    程序使用的参数如\cref{tbl:exp3para}所示，其中$\alpha$值的区别是经过试探得到的，在L3、LM方法中使用更大的$\alpha$也可以保证收敛，同时提高了收敛速度。运行程序并统计结果，如\cref{tbl:exp3}所示。迭代过程中RMSE和目标函数J值的变化如\cref{fig:exp3_k50l2,fig:exp3_l3_k50l2,fig:exp3_lm_k50l2}所示。

    \begin{table}
      \centering
      \caption{算例所采用的参数表}
      \label{tbl:exp3para}
      \begin{tabular}{lr}
        \toprule
        参数&值\\
        \midrule
        $\lambda$ & $10^{-2}$ \\
        $k$ & $50$ \\
        $\alpha$ & Original \& Gauss: $10^{-4}$\\
         & L3 \& LM: $5\times 10^{-5}$ \\
        $\epsilon$ & $10^{3}$ \\
        $e$ & $10^{-2}$ \\
        \bottomrule
      \end{tabular}
    \end{table}

    \begin{table}
      \centering
      \caption{矩阵分解算法结果对比}
      \label{tbl:exp3}
      \begin{tabular}{lrrrr}
        \toprule
        算法&平均单步时间&迭代次数&总时间&RMSE\\
        \midrule
        Original & - & 153 & - & 0.907 \\
        Gauss & 4299.47ms & 123 & 528534.81ms & 0.918 \\
        L3 & 2580.86ms & 213 & 394871.58ms & 0.807 \\
        LM & 2809.87ms & 244 & 433959.46ms & 0.810 \\
        \bottomrule
      \end{tabular}
    \end{table}

    \begin{figure}[htbp]
      \centering
      \includegraphics[width=0.9\linewidth]{exp3_combined_k50l2}
      \caption{Jacobi/Gauss-Seidel矩阵分解法RMSE与J变化曲线($k=50,\lambda=0.01$)}
      \label{fig:exp3_k50l2}
    \end{figure}
    \begin{figure}[htbp]
      \centering
      \includegraphics[width=0.9\linewidth]{exp3_L3}
      \caption{L3-Jacobi矩阵分解法RMSE与J变化曲线}
      \label{fig:exp3_l3}
    \end{figure}
    \begin{figure}[htbp]
      \centering
      \includegraphics[width=0.9\linewidth]{exp3_LM}
      \caption{LM-Jacobi矩阵分解法RMSE与J变化曲线}
      \label{fig:exp3_lm}
    \end{figure}

    特别关注作业要求分析的\cref{fig:exp3_k50l2}。可见，目标函数值和RMSE的下降趋势基本相同，这说明所设计的目标函数是合理的，的确能够反映隐空间对原矩阵的拟合效果。在开始的几步时，目标函数下降较慢，而在5步以后，目标函数值开始快速下降，直到收敛至最终值附近，下降速度开始放缓并触发收敛判据。

    \cref{fig:exp3_k50l2}对比Jacobi迭代和Gauss-Seidel迭代可见，后者的收敛速度的确较前者更快，且二者都能收敛到相近的目标函数值及解上。然而，通过\cref{tbl:exp3}可知，Gauss-Seidel法下降更快导致更早触发收敛判据，尽管目标函数值已经收敛，其对原矩阵的拟合效果并不好。另外，虽然Gauss-Seidel的迭代步数更少，但其在每一步中都需要重新计算$UV^T$，导致运算效率大大降低，总体时间更长。归根结底，Gauss-Seidel方法不够出色的原因，是我们只有两个变量UV，无法体现其优势，并且由于单次计算量大，Jacobi带来的缓存优化更胜一筹。因此，在本题中使用Jacobi方法更为合适，对于L3方法和LM方法的测试，都是基于Jacobi方法的。

    注意到，\cref{fig:exp3_l3_k50l2,fig:exp3_lm_k50l2}中，RMSE曲线有尾部上翘现象，意味着在迭代过程中目标函数收敛之前，RMSE曾达到更小的值。由于我们不应使用任何测试集的信息，因此只能拿到最后的RMSE。这个问题，一方面是由于迭代次数过多后，出现过拟合现象(TODO说明过拟合)；另一方面是由于目标函数的设计和当前问题的拟合效果不好，如果改善目标函数，例如调整$\lambda$参数，情况将有所改善。在\cref{sub:param}中优化得到的参数$k=70,\lambda=10$得到的曲线在\cref{fig:exp3_l3,fig:exp3_lm}用虚线展示，可见尾部上翘现象已经消除。

    \subsection{参数优化}
    \label{sub:param}
    作业中的默认参数$k=50,\lambda=0.01$，已经可以达到很好的收敛效果和预测效果。为了找到更好的参数，我们对原参数附近的值进行试探扫描。经过设计，我们扫描的区间是$k=\{10,30,50,70,90\},\lambda=\{0.001,0.01,0.1,10,100\}$，得到结果如\cref{tbl:paramRes,tbl:paramResL3,tbl:paramResLM}所示。

    \begin{table}[htbp]
      \centering
      \caption{原始矩阵分解算法参数优化}
      \label{tbl:paramRes}
      \include{meta/paramRes}
    \end{table}
    \begin{table}[htbp]
      \centering
      \caption{L3矩阵分解算法参数优化}
      \label{tbl:paramResL3}
      \include{meta/paramResL3}
    \end{table}
    \begin{table}[htbp]
      \centering
      \caption{LM矩阵分解算法参数优化}
      \label{tbl:paramResLM}
      \include{meta/paramResLM}
    \end{table}

    因此，我们可以得到优化后的参数如下：
    \begin{enumerate}
      \item 原方法：$k=70,\lambda=10$，结果为$RMSE=0.888$，203步收敛。
      \item L3方法：$k=70,\lambda=10$，结果为$RMSE=0.796$，244步收敛。
      \item LM方法：$k=70,\lambda=10$，结果为$RMSE=0.800$，246步收敛。
    \end{enumerate}

    可见，在本报告提出的方法和原方法中，$k=70,\lambda=10$都是一组更好的参数。另外，在矩阵分解方法中，L3方法的表现从收敛速度、最终结果上都要优于LM算法，这是出乎意料的。
    \section{方法对比}
    TODO

    \label{applastpage}
    \newpage
    \bibliographystyle{ieeetr}
    \bibliography{report}
\iffalse
\begin{itemize}[noitemsep,topsep=0pt]
%no white space
\end{itemize}
\begin{enumerate}[label=\Roman{*}.,noitemsep,topsep=0pt]
%use upper case roman
\end{enumerate}
\begin{multicols}{2}
%two columns
\end{multicols}
\fi
\end{document}
